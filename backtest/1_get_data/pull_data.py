from datetime import datetime, timedelta
import multiprocessing as mp
import pickle

import numpy as np
import pandas as pd
import yfinance as yf


DATA = '../data'
START_DATE = '1950-01-01'
TOMORROW = (datetime.now() + timedelta(1)).date()
N_JOBS = mp.cpu_count()
OUTPATH = f'{DATA}/basic_stock_data.csv'
BATCH_SIZE = 10


def main():
    StockGetter().get_stocks()


class StockGetter:
    def __init__(self):
        self.batches = self._get_batches()
        
    def _get_batches(self):
        symbols = self._get_symbols()
        n = len(symbols)
        batches = []
        batch_start = 0
        batch_end = min(batch_start + BATCH_SIZE, n)
        while batch_start < n:
            batches.append(symbols[batch_start:batch_end])
            batch_start = batch_end
            batch_end = min(batch_start + BATCH_SIZE, n)
        print(
            f'Stocks batched into {len(batches)} batches of up to '
            f'{BATCH_SIZE} each.')
        return batches

    @staticmethod
    def _get_symbols() -> list[str]:
        # Note: the following file is generated by running the
        # /notebooks/stock_symbol_scrape.py.ipynb notebook
        with open(f'{DATA}/all_symbols.pkl', 'rb') as f:
            symbols = pickle.load(f)
        print(f'Retrieved {len(symbols)} symbols.')
        return symbols

    def get_stocks(self, run_batches: list[int] | None = None):
        # In order to prevent getting rate-limited, we get data in batches of
        # 100 stocks sequentially (rather than multiprocessing or
        # multithreading). Each batch is enumerated so that just specific
        # batches can be rerun in the event of failure.
        # The data are pulled and immediately written to disk
        for i, batch in enumerate(self.batches):
            #try:
            self._download_batch(i, batch)
            #except KeyError:
            #    pass
            #    print(f'Unexpected error for batch {i}:\n{e}')
            #return
            break
        print('Downloads complete.')

    # TODO: fill gap NAs, refactor, multiprocess this method
    def _download_batch(self, i, batch):
        new_cols = []
        df = (
            yf.download(batch, start=START_DATE, end=TOMORROW)
            .drop(columns='Open'))
        null_sum = df.isnull().sum()
        empties = null_sum[null_sum == len(df)].index
        df.drop(columns=empties, inplace=True)
        df.columns = df.columns.map(
            lambda t: f'{t[1]}_{t[0]}'.lower().replace('_adj close', ''))
        for symbol in batch:
            symbol = symbol.lower()
            try:
                adj_factor = df[symbol] / df[f'{symbol}_close']
                df[f'{symbol}_high'] *= adj_factor
                df[f'{symbol}_low'] *= adj_factor
                pct_change = self._get_pct_change(
                    symbol, df[symbol])
                next_day_low, next_day_high = self._get_next_day_ranges(
                    symbol,
                    df[symbol],
                    df[f'{symbol}_low'],
                    df[f'{symbol}_high'])
                new_cols += [pct_change, next_day_low, next_day_high]
            except KeyError:
                print(f'Did not find column for {symbol}. Skipping.')
        df = pd.concat([df, *new_cols], axis=1)
        drops = [
            c for c in df.columns if c.endswith('_close')
            or c.endswith('_high')
            or c.endswith('_low')]
        df.drop(columns=drops, inplace=True)
        df.index = df.index.astype(str)
        path = f'{DATA}/raw_stocks_batch_{i}_{batch[0]}_{batch[-1]}.csv'
        df.to_csv(path)
        print('Wrote batch to', path)    
        return df

    @staticmethod
    def _get_pct_change(symbol: str, stock: pd.Series) -> pd.Series:
        return pd.Series(
            np.concatenate([[np.nan], stock.values[1:] / stock.values[:-1]]),
            index=stock.index,
            name=f'{symbol}_pct_change')

    @staticmethod
    def _get_next_day_ranges(
            symbol: str, close: pd.Series, low: pd.Series, high: pd.Series):
        c = close.values[:-1]
        l = low.values[1:]
        h = high.values[1:]
        next_day_low = pd.Series(
            np.concatenate([[np.nan], l / c]),
            index=close.index,
            name=f'{symbol}_next_day_min_mult')
        next_day_high = pd.Series(
            np.concatenate([[np.nan], h / c]),
            index=close.index,
            name=f'{symbol}_next_day_max_mult')
        return next_day_low, next_day_high
               




if __name__ == '__main__':
    main()
